---
layout: default
title: ORNL's AI Seminar Series <br/> 
description: Organized by the ORNL AI Initiative <br/>
             10am - 11am ET <br/> 
             Thursdays, January-December, 2026 <br/> 
             Hybrid (Onsite & Virtual) <br/>
             
permalink: /events/ai-initiative-seminar-2026/
tags: events
---

# About

The ORNL AI Seminar Series (Biweekly/Hybrid), organized by the [AI Initiative](https://www.ornl.gov/ai), serves as a platform for researchers and engineers from diverse scientific, engineering, and national security backgrounds spanning ORNL, universities, and industry.
Our main objective is to encourage collaboration with the goal of driving transformative advancements in safe, trustworthy, and energy-efficient AI research and its applications.

The seminar will be held on Thursdays from 10 am to 11am ET.
Please reach out to the organizers if you would like to recommend a spearker or give a talk.

<a href="#top"> &#10558; Back to top</a>

# Next Presentations

**Efficient Low-Rank Training and Fine-Tuning of Neural Networks**

<br>Location: Building 5700 and MS Teams
<br> Time: 10:00 a.m. - 11 a.m. ET, Thursday, 01/22/2026
<br>Speaker: [Steffen Schotthoefer](https://www.ornl.gov/staff-profile/steffen-schotthoefer)

|         |
| ------- |
| <img src="https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2024-06/ornl_profile_picture_1.jpg" width="200" /> |
| Steffen Schotthoefer <br> Alston S. Householder Fellow, Computer Science and Mathematics Division <br>ORNL |

**Abstract**
Low-rank adaptation (LoRA) has become the de-facto state-of-the-art method for parameter efficient fine-tuning of large-scale, pre-trained neural networks.  Similarly, low-rank compression of pre-trained networks has become a widely adopted technique to reduce the parameter count of networks for fast inference on resource constraint devices.  The idea of low-rank methods is based upon the assumption that the weight matrices of overparametrized neural networks are of low-rank.  Thus, a factorization of the weight layers based on truncated singular value decompositions can be employed to reduce the memory footprint of the network.  However, LoRA and its extensions face several challenges in practice, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process.  In this talk, Dr. Schotthoefer investigates mathematical concepts of low-rank training and uses the gained insights to design efficient and robust low-rank training algorithms.

**Speaker's Bio**
Dr. Steffen Schotthoefer is the current Householder Fellow in the Mathematics in Computation Section at the Oak Ridge National Laboratory (ORNL), affiliated with the Multiscale Methods and Dynamics Group.  Steffen's work centers on creating efficient numerical methods for training and fine-tuning artificial intelligence models in environments with limited resources and at large scales.  He investigates low-rank methods for model compression to minimize the computational cost of neural network training and inference.  In addition, Steffen develops neural network-based surrogate models for scientific domains such as radiation transport and plasma dynamics.  His research aims to tackle the challenges posed by memory and communication bottlenecks in large-scale simulations.  Prior to joining ORNL, Steffen completed his Ph.D. in Applied Mathematics at Karlsruhe Institute of Technology, Germany, focusing on neural network-based surrogate modeling for radiation transport. 

<a href="#top"> &#10558; Back to top</a>

---

# Schedule

<!---
The table should be update routein to reflect the upcoming events, and the past events should be at the bottom of the table.
-->

Please reach out if you are interested in presenting at a future event

|      Date      |    Location    |        Name            |          Affilication           |      Talk      |
| :------------: | :------------: | :--------------------: | :-----------------------------: | :------------: |
| 01-22-2026 | On Site | Steffen Schotthoefer | ORNL | Efficient Low-Rank Training and Fine-Tuning of Neural Networks |
| 01-29-2026 | On Site | Stefan Schnake | ORNL | TBD |

<a href="#top"> &#10558; Back to top</a>

# Past Presentations

**Designing Explainable Neural NetworksWith Physics Constraints via Optimization**

<br>Location: MS Teams
<br> Time: 11:00 a.m. - 12 p.m. ET, Thursday, 11/20/2025
<br>Speaker: [Samy Wu Fung](https://swufung.github.io/)

|         |
| ------- |
| <img src="https://swufung.github.io/images/profile_photo.png" width="200" /> |
| Samy Wu Fung <br> Assistant Professor, Department of Applied Mathematics and Statistics <br>Colorado School of Mines |

**Abstract**
This talk explores designing neural networks that enforce physics principles as hard constraints through a class of network architectures known as implicit neural networks. These networks produce outputs satisfying specific feasibility and optimality conditions that represent domain/physics knowledge. Samy will address the challenges of training these networks efficiently, illustrate some applications where they have found success, and provide a bound on how well they generalize.

**Speaker's Bio**
Samy Wu Fung is an Assistant Professor in the Department of Applied Mathematics and Statistics at Colorado School of Mines. Prior to joining Mines, he was an Assistant Adjunct Professor at University of California Los Angeles. He obtained his Ph.D. from Emory University, working on large-scale geophysical inverse problems and machine learning. His research interests are at the intersection of inverse problems and optimization, optimal control, mean field games, and deep learning.  

---

<a href="#top"> &#10558; Back to top</a>

# Organization

For questions, please contact us.
<style>
td, th {
   border: none!important;
}
</style>

|                |                |                |                |
| -------------- | -------------- | -------------- | -------------- |
| [![Yan Liu](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2019-04/liuy8.png?h=5114cd9b&itok=5Nt4keCd)](https://www.ornl.gov/staff-profile/yan-liu) | [![Jong Youl Choi](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2021-02/jychoi2_0.png?h=273942d0&itok=wF9lLEZU)](https://www.ornl.gov/staff-profile/jong-youl-choi) | [![Chen Zhang](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2020-10/profile_0.png?h=c49a1206&itok=ntQg6NeU)](https://www.ornl.gov/staff-profile/chen-zhang) | [![Sudip Seal](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2018-P01367.jpg)](https://www.ornl.gov/staff-profile/sudip-k-seal) |
| Yan Liu <br> Senior Computational Scientist <br> Computational Sciences & Engineering Division <br> ORNL | Jong Youl Choi <br> HPC Data Research Scientist <br> Computer Science and Mathematics Division <br> ORNL | Chen Zhang <br> Computational Scientist <br> Computer Science and Mathematics Division <br> ORNL | Sudip Seal<br> Director, AI Initiative<br> Distinguished R&D Staff Scientist<br> Computer Science and Mathematics Division, ORNL |

<a href="#top"> &#10558; Back to top</a>
