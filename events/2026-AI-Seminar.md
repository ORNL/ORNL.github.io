---
layout: default
title: ORNL's AI Seminar Series <br/> 
description: Organized by the ORNL AI Initiative <br/>
             10am - 11am ET <br/> 
             Thursdays, January-December, 2026 <br/> 
             Hybrid (Onsite & Virtual) <br/>
             
permalink: /events/ai-initiative-seminar-2026/
tags: events
---

# About

The ORNL AI Seminar Series (Biweekly/Hybrid), organized by the [AI Initiative](https://www.ornl.gov/ai), serves as a platform for researchers and engineers from diverse scientific, engineering, and national security backgrounds spanning ORNL, universities, and industry.
Our main objective is to encourage collaboration with the goal of driving transformative advancements in safe, trustworthy, and energy-efficient AI research and its applications.

The seminar will be held on Thursdays from 10 am to 11am ET.
Please reach out to the organizers if you would like to recommend a spearker or give a talk.

<a href="#top"> &#10558; Back to top</a>

# Next Presentations

**Dynamical low-rank compression of neural networks with robustness under adversarial attacks**

<br>Location: Room F234, Building 5700 and MS Teams
<br> Time: 10:00 a.m. - 11 a.m. ET, Thursday, 01/29/2026
<br>Speaker: [Stefan Schnake](https://www.ornl.gov/staff-profile/stefan-r-schnake)

|         |
| ------- |
| <img src="https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2021-03/IMG_6786.jpg" width="200" /> |
| Stefan Schnake <br> Research Scientist, Computer Science and Mathematics Division <br>ORNL |

**Abstract**
Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing accuracy on clean data. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over `94%` compression while recovering or improving adversarial accuracy relative to uncompressed baselines.

**Speaker's Bio**
Dr. Stefan Schnake is a research scientist in the Multiscale Methods and Dynamics Group.  He received his Ph.D. in 2017 from the University of Tennessee and joined Oak Ridge National Laboratory in 2020 after a postdoctoral appointment at the University of Oklahoma.  His research interests include low-rank and sparse-grid methods for compressing tensor representations of solutions to dynamical systems.  Applications of his research include machine learning and high-dimensional partial differential equation approximation.

<a href="#top"> &#10558; Back to top</a>

---

# Schedule

<!---
The table should be update routein to reflect the upcoming events, and the past events should be at the bottom of the table.
-->

Please reach out if you are interested in presenting at a future event

|      Date      |    Location    |        Name            |          Affilication           |      Talk      |
| :------------: | :------------: | :--------------------: | :-----------------------------: | :------------: |
| 01-22-2026 | On Site | Steffen Schotthoefer | ORNL | Efficient Low-Rank Training and Fine-Tuning of Neural Networks |
| 01-29-2026 | On Site | Stefan Schnake | ORNL | Dynamical low-rank compression of neural networks with robustness under adversarial attacks |

<a href="#top"> &#10558; Back to top</a>

# Past Presentations

**Efficient Low-Rank Training and Fine-Tuning of Neural Networks**

<br>Location: Room F234 Building 5700 and MS Teams
<br> Time: 10:00 a.m. - 11 a.m. ET, Thursday, 01/22/2026
<br>Speaker: [Steffen Schotthoefer](https://www.ornl.gov/staff-profile/steffen-schotthoefer)

|         |
| ------- |
| <img src="https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2024-06/ornl_profile_picture_1.jpg" width="200" /> |
| Steffen Schotthoefer <br> Alston S. Householder Fellow, Computer Science and Mathematics Division <br>ORNL |

**Abstract**
Low-rank adaptation (LoRA) has become the de-facto state-of-the-art method for parameter efficient fine-tuning of large-scale, pre-trained neural networks.  Similarly, low-rank compression of pre-trained networks has become a widely adopted technique to reduce the parameter count of networks for fast inference on resource constraint devices.  The idea of low-rank methods is based upon the assumption that the weight matrices of overparametrized neural networks are of low-rank.  Thus, a factorization of the weight layers based on truncated singular value decompositions can be employed to reduce the memory footprint of the network.  However, LoRA and its extensions face several challenges in practice, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process.  In this talk, Dr. Schotthoefer investigates mathematical concepts of low-rank training and uses the gained insights to design efficient and robust low-rank training algorithms.

**Speaker's Bio**
Dr. Steffen Schotthoefer is the current Householder Fellow in the Mathematics in Computation Section at the Oak Ridge National Laboratory (ORNL), affiliated with the Multiscale Methods and Dynamics Group.  Steffen's work centers on creating efficient numerical methods for training and fine-tuning artificial intelligence models in environments with limited resources and at large scales.  He investigates low-rank methods for model compression to minimize the computational cost of neural network training and inference.  In addition, Steffen develops neural network-based surrogate models for scientific domains such as radiation transport and plasma dynamics.  His research aims to tackle the challenges posed by memory and communication bottlenecks in large-scale simulations.  Prior to joining ORNL, Steffen completed his Ph.D. in Applied Mathematics at Karlsruhe Institute of Technology, Germany, focusing on neural network-based surrogate modeling for radiation transport. 

---

<a href="#top"> &#10558; Back to top</a>

# Organization

For questions, please contact us.
<style>
td, th {
   border: none!important;
}
</style>

|                |                |                |                |                |
| -------------- | -------------- | -------------- | -------------- | -------------- |
| [![Yan Liu](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2019-04/liuy8.png?h=5114cd9b&itok=5Nt4keCd)](https://www.ornl.gov/staff-profile/yan-liu) | [![Jong Youl Choi](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2021-02/jychoi2_0.png?h=273942d0&itok=wF9lLEZU)](https://www.ornl.gov/staff-profile/jong-youl-choi) | [![Lexie Yang](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2020-03/2020-P01810_0.jpg)](https://www.ornl.gov/staff-profile/lexie-yang) | [![Chen Zhang](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2020-10/profile_0.png?h=c49a1206&itok=ntQg6NeU)](https://www.ornl.gov/staff-profile/chen-zhang) | [![Sudip Seal](https://www.ornl.gov/sites/default/files/styles/staff_profile_image_style/public/2018-P01367.jpg)](https://www.ornl.gov/staff-profile/sudip-k-seal) |
| Yan Liu <br> Senior Computational Scientist, <br> CSED, ORNL | Jong Youl Choi <br> HPC Data Research Scientist, <br> CSMD, ORNL | Lexie Yang <br> Senior R&D Staff Scientist, <br> GSHS, ORNL | Chen Zhang <br> Computational Scientist, <br> CSMD, ORNL | Sudip Seal<br> Director, AI Initiative<br> Distinguished R&D Staff Scientist<br> CSMD, ORNL |

<a href="#top"> &#10558; Back to top</a>
